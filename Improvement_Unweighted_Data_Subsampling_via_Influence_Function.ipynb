{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8JsMrkCrYw3"
      },
      "source": [
        "## **Basic imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSxkqZgWOTps",
        "outputId": "8e15492c-3bce-42b4-d484-2fd2e291561e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/Influence_Subsampling'\n",
            "/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Influence_Subsampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHQ8PmdiOarB",
        "outputId": "e9c54334-d8f9-4c65-ffe5-63a1b86e6c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improvement_Unweighted_Data_Subsampling_via_Influence_Function.ipynb\n",
            "improvement_unweighted_data_subsampling_via_influence_function.py\n",
            "inverse_hvp.py\n",
            "optimize.py\n",
            "__pycache__\n",
            "README.md\n",
            "t10k-images-idx3-ubyte.gz\n",
            "t10k-labels-idx1-ubyte.gz\n",
            "train-images-idx3-ubyte.gz\n",
            "train-labels-idx1-ubyte.gz\n",
            "Unweighted_Data_Subsampling_via_Influence_Function.ipynb\n",
            "unweighted_data_subsampling_via_influence_function.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aIh_f_kK1G_g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from inverse_hvp import inverse_hvp_lr_newtonCG\n",
        "import argparse\n",
        "import time\n",
        "import pdb\n",
        "import os\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AdBdltarAyJ"
      },
      "source": [
        "# 1. **MNIST Dataset Load and Preprocessing the data in required format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CV4aHBR41G_x"
      },
      "outputs": [],
      "source": [
        "# select the dataset used\n",
        "dataset_name = \"mnist\"\n",
        "# regularization parameter for Logistic Regression\n",
        "C = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOGuUulj3n4h",
        "outputId": "e202c0ee-05c1-4a31-b1a1-a9a4d10feda7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KFold(n_splits=3, random_state=None, shuffle=False)\n",
            "x_te,    nr sample 2163, nr feature 784\n",
            "Te: Pos 1135 Neg 1028\n",
            " for k fold 0 -------------------->\n",
            "x_train, nr sample 7928, nr feature 784\n",
            "x_va,    nr sample 3964, nr feature 784\n",
            "Tr: Pos 4084 Neg 3844\n",
            "Va: Pos 2093 Neg 1871\n",
            " for k fold 1 -------------------->\n",
            "x_train, nr sample 11893, nr feature 784\n",
            "x_va,    nr sample 3964, nr feature 784\n",
            "Tr: Pos 6178 Neg 5715\n",
            "Va: Pos 2089 Neg 1875\n",
            " for k fold 2 -------------------->\n",
            "x_train, nr sample 7929, nr feature 784\n",
            "x_va,    nr sample 3963, nr feature 784\n",
            "Tr: Pos 4183 Neg 3746\n",
            "Va: Pos 1995 Neg 1968\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# tool box\n",
        "def load_mnist(validation_size = 5000):\n",
        "    import gzip\n",
        "    def _read32(bytestream):\n",
        "        dt = np.dtype(np.uint32).newbyteorder(\">\")\n",
        "        return np.frombuffer(bytestream.read(4),dtype=dt)[0]\n",
        "\n",
        "    def extract_images(f):\n",
        "        with gzip.GzipFile(fileobj=f) as bytestream:\n",
        "            magic = _read32(bytestream)\n",
        "            num_images = _read32(bytestream)\n",
        "            rows = _read32(bytestream)\n",
        "            cols = _read32(bytestream)\n",
        "            buf = bytestream.read(rows * cols * num_images)\n",
        "            data = np.frombuffer(buf,dtype=np.uint8)\n",
        "            data = data.reshape(num_images,rows,cols,1)\n",
        "            return data\n",
        "    \n",
        "    def extract_labels(f):\n",
        "        with gzip.GzipFile(fileobj=f) as bytestream:\n",
        "            magic = _read32(bytestream)\n",
        "            num_items = _read32(bytestream)\n",
        "            buf = bytestream.read(num_items)\n",
        "            labels = np.frombuffer(buf, dtype=np.uint8)\n",
        "            return labels\n",
        "\n",
        "    data_dir = \"./improvement\"\n",
        "    TRAIN_IMAGES = os.path.join(data_dir,'train-images-idx3-ubyte.gz')\n",
        "    with open(TRAIN_IMAGES,\"rb\") as f:\n",
        "        train_images = extract_images(f)\n",
        "\n",
        "    TRAIN_LABELS =  os.path.join(data_dir,'train-labels-idx1-ubyte.gz')\n",
        "    with open(TRAIN_LABELS,\"rb\") as f:\n",
        "        train_labels = extract_labels(f)\n",
        "\n",
        "    TEST_IMAGES =  os.path.join(data_dir,'t10k-images-idx3-ubyte.gz')\n",
        "    with open(TEST_IMAGES,\"rb\") as f:\n",
        "        test_images = extract_images(f)\n",
        "\n",
        "    TEST_LABELS =  os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "    with open(TEST_LABELS,\"rb\") as f:\n",
        "        test_labels = extract_labels(f)\n",
        "\n",
        "    # split train and val\n",
        "    train_images = train_images[validation_size:]\n",
        "    train_labels = train_labels[validation_size:]\n",
        "\n",
        "    # preprocessing\n",
        "    train_images = train_images.astype(np.float32) / 255\n",
        "    test_images  = test_images.astype(np.float32) / 255\n",
        "    \n",
        "    # reshape for logistic regression\n",
        "    train_images = np.reshape(train_images, [train_images.shape[0], -1])\n",
        "    test_images = np.reshape(test_images, [test_images.shape[0], -1])\n",
        "    return train_images,train_labels,test_images,test_labels\n",
        "\n",
        "\n",
        "def filter_dataset(X, Y, pos_class, neg_class, mode=None):\n",
        "    \n",
        "    \"\"\"\n",
        "    Filters out elements of X and Y that aren't one of pos_class or neg_class\n",
        "    then transforms labels of Y so that +1 = pos_class, -1 = neg_class.\n",
        "\n",
        "    They are all 10-classes image classification data sets while Logistic regression can only handle binary classification. we\n",
        "    select the number 1 and 7 as positive and negative classes;\n",
        "    \"\"\"\n",
        "\n",
        "    assert(X.shape[0] == Y.shape[0])\n",
        "    assert(len(Y.shape) == 1)\n",
        "\n",
        "    Y = Y.astype(int)\n",
        "    \n",
        "    pos_idx = Y == pos_class\n",
        "    neg_idx = Y == neg_class        \n",
        "    Y[pos_idx] = 1\n",
        "    Y[neg_idx] = -1\n",
        "    idx_to_keep = pos_idx | neg_idx\n",
        "    X = X[idx_to_keep, ...]\n",
        "    Y = Y[idx_to_keep]\n",
        "    if Y.min() == -1 and mode != \"svm\":\n",
        "        Y = (Y + 1) / 2\n",
        "        Y.astype(int)\n",
        "    return (X, Y)\n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "def load_data_two_class(dataset_name,va_ratio):\n",
        "  x_train,y_train,x_test,y_test = load_mnist()\n",
        "  pos_class = 1\n",
        "  neg_class = 7\n",
        "  x_train,y_train = filter_dataset(x_train,y_train,pos_class,neg_class)\n",
        "  x_va,y_va = filter_dataset(x_test,y_test,pos_class,neg_class)\n",
        "  y_va = y_va.astype(int)\n",
        "  y_train = y_train.astype(int)\n",
        "\n",
        "  ######################################## 3 fold cross validation for improvement  #########################################\n",
        "  kf = KFold(n_splits=3) # Define the split - into 3 folds \n",
        "  kf.get_n_splits(x_train) # returns the number of splitting iterations in the cross-validator\n",
        "  print(kf) \n",
        "  KFold(n_splits=3, random_state=None, shuffle=False)\n",
        "  i=0\n",
        "\n",
        "  for train_index, test_index in kf.split(x_train):\n",
        "    if (i==0):\n",
        "      x_train_0 = x_train[train_index[0]:train_index[-1]]\n",
        "      y_train_0 = y_train[train_index[0]:train_index[-1]]\n",
        "      x_val_0 = x_train[test_index[0]:test_index[-1]]\n",
        "      y_val_0 = y_train[test_index[0]:test_index[-1]]\n",
        "\n",
        "    elif (i==1):\n",
        "      x_train_1 = x_train[train_index[0]:train_index[-1]]\n",
        "      y_train_1 = y_train[train_index[0]:train_index[-1]]\n",
        "      x_val_1 = x_train[test_index[0]:test_index[-1]]\n",
        "      y_val_1 = y_train[test_index[0]:test_index[-1]]\n",
        "\n",
        "    elif (i==2):\n",
        "      x_train_2 = x_train[train_index[0]:train_index[-1]]\n",
        "      y_train_2 = y_train[train_index[0]:train_index[-1]]\n",
        "      x_val_2 = x_train[test_index[0]:test_index[-1]]\n",
        "      y_val_2 = y_train[test_index[0]:test_index[-1]]\n",
        "\n",
        "    i=i+1\n",
        "    \n",
        "  ############################################################################################################################\n",
        "  \n",
        "  x_te = x_va\n",
        "  y_te = y_va\n",
        "\n",
        "  return x_train_0,y_train_0,x_val_0,y_val_0,x_train_1,y_train_1,x_val_1,y_val_1,x_train_2,y_train_2,x_val_2,y_val_2,x_te,y_te\n",
        "\n",
        "# load data, pick 3 cross validation as the Va set\n",
        "x_train_0,y_train_0,x_va_0,y_va_0,x_train_1,y_train_1,x_va_1,y_va_1,x_train_2,y_train_2,x_va_2,y_va_2,x_te,y_te = load_data_two_class(dataset_name,va_ratio=0.3)\n",
        "\n",
        "\n",
        "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
        "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))\n",
        "\n",
        "print(\" for k fold 0 -------------------->\")\n",
        "print(\"x_train, nr sample {}, nr feature {}\".format(x_train_0.shape[0],x_train_0.shape[1]))\n",
        "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va_0.shape[0],x_va_0.shape[1]))\n",
        "print(\"Tr: Pos {} Neg {}\".format(y_train_0[y_train_0==1].shape[0],y_train_0[y_train_0==0].shape[0]))\n",
        "print(\"Va: Pos {} Neg {}\".format(y_va_0[y_va_0==1].shape[0],y_va_0[y_va_0==0].shape[0]))\n",
        "\n",
        "print(\" for k fold 1 -------------------->\")\n",
        "print(\"x_train, nr sample {}, nr feature {}\".format(x_train_1.shape[0],x_train_1.shape[1]))\n",
        "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va_1.shape[0],x_va_1.shape[1]))\n",
        "print(\"Tr: Pos {} Neg {}\".format(y_train_1[y_train_1==1].shape[0],y_train_1[y_train_1==0].shape[0]))\n",
        "print(\"Va: Pos {} Neg {}\".format(y_va_1[y_va_1==1].shape[0],y_va_1[y_va_1==0].shape[0]))\n",
        "\n",
        "print(\" for k fold 2 -------------------->\")\n",
        "print(\"x_train, nr sample {}, nr feature {}\".format(x_train_2.shape[0],x_train_2.shape[1]))\n",
        "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va_2.shape[0],x_va_2.shape[1]))\n",
        "print(\"Tr: Pos {} Neg {}\".format(y_train_2[y_train_2==1].shape[0],y_train_2[y_train_2==0].shape[0]))\n",
        "print(\"Va: Pos {} Neg {}\".format(y_va_2[y_va_2==1].shape[0],y_va_2[y_va_2==0].shape[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIS_ygsKtHM2"
      },
      "source": [
        "# 2. **Flip labels for some of the images to make the data noisy so that we can show how our method discrad these kind of noisy data hence flipping the labels of some data in below code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KZ26meoM1HAF"
      },
      "outputs": [],
      "source": [
        "# get the subset samples number\n",
        "num_tr_sample = x_train_0.shape[0]\n",
        "sample_ratio = 0.6\n",
        "obj_sample_size = int(sample_ratio * num_tr_sample)\n",
        "\n",
        "\"\"\"\n",
        "Our unweighted method can downweight the bad cases which cause high test loss to the our model, which is an important reason of its ability to improve result with less data.\n",
        "To show the performance of our methods in noisy label situation, we perform addtional experiments with some training\n",
        "labels being flipped. The results show the enlarging superiority of our subsampling methods \n",
        "\"\"\"\n",
        "\n",
        "# flip labels\n",
        "idxs = np.arange(y_train_0.shape[0])\n",
        "flip_ratio = 0.4\n",
        "np.random.shuffle(idxs)\n",
        "num_flip = int(flip_ratio * len(idxs))\n",
        "y_train_0[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train_0[idxs[:num_flip]]).astype(int)\n",
        "\n",
        "idxs = np.arange(y_train_1.shape[0])\n",
        "np.random.shuffle(idxs)\n",
        "num_flip = int(flip_ratio * len(idxs))\n",
        "y_train_1[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train_1[idxs[:num_flip]]).astype(int)\n",
        "\n",
        "idxs = np.arange(y_train_2.shape[0])\n",
        "flip_ratio = 0.4\n",
        "np.random.shuffle(idxs)\n",
        "num_flip = int(flip_ratio * len(idxs))\n",
        "y_train_2[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train_2[idxs[:num_flip]]).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65j1_ru2txsq"
      },
      "source": [
        "# 3. **Training model on the full data set (ˆθ on the full Tr)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AvhPW3cFtx89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LibLinear]iter  1 act 1.153e+01 pre 1.142e+01 delta 2.548e+00 f 5.495e+02 |g| 3.247e+01 CG  18\n",
            "iter  2 act 4.295e-01 pre 4.295e-01 delta 2.548e+00 f 5.380e+02 |g| 2.854e+00 CG  25\n",
            "iter  3 act 2.473e-03 pre 2.473e-03 delta 2.548e+00 f 5.376e+02 |g| 2.193e-01 CG  27\n",
            "iter  4 act 1.101e-05 pre 1.101e-05 delta 2.548e+00 f 5.376e+02 |g| 1.408e-02 CG  26\n",
            "iter  5 act 8.772e-08 pre 8.772e-08 delta 2.548e+00 f 5.376e+02 |g| 1.136e-03 CG  29\n",
            "iter  6 act 1.970e-10 pre 1.945e-10 delta 2.548e+00 f 5.376e+02 |g| 9.765e-05 CG  19\n",
            "WARNING: actred and prered too small\n",
            "[LibLinear]iter  1 act 1.228e+01 pre 1.218e+01 delta 2.099e+00 f 8.244e+02 |g| 6.897e+01 CG  17\n",
            "iter  2 act 1.047e+00 pre 1.047e+00 delta 2.099e+00 f 8.121e+02 |g| 6.070e+00 CG  25\n",
            "iter  3 act 1.545e-02 pre 1.545e-02 delta 2.099e+00 f 8.110e+02 |g| 5.956e-01 CG  28\n",
            "iter  4 act 1.442e-04 pre 1.442e-04 delta 2.099e+00 f 8.110e+02 |g| 4.672e-02 CG  33\n",
            "iter  5 act 6.931e-07 pre 6.931e-07 delta 2.099e+00 f 8.110e+02 |g| 4.446e-03 CG  28\n",
            "iter  6 act 7.704e-09 pre 7.699e-09 delta 2.099e+00 f 8.110e+02 |g| 4.172e-04 CG  33\n",
            "iter  7 act 2.467e-11 pre 3.077e-11 delta 2.099e+00 f 8.110e+02 |g| 2.926e-05 CG  25\n",
            "WARNING: actred and prered too small\n",
            "[LibLinear]iter  1 act 1.080e+01 pre 1.069e+01 delta 2.173e+00 f 5.496e+02 |g| 4.608e+01 CG  17\n",
            "iter  2 act 5.314e-01 pre 5.314e-01 delta 2.173e+00 f 5.388e+02 |g| 4.874e+00 CG  22\n",
            "iter  3 act 5.942e-03 pre 5.942e-03 delta 2.173e+00 f 5.383e+02 |g| 3.566e-01 CG  25\n",
            "iter  4 act 4.928e-05 pre 4.927e-05 delta 2.173e+00 f 5.383e+02 |g| 3.055e-02 CG  27\n",
            "iter  5 act 2.014e-07 pre 2.014e-07 delta 2.173e+00 f 5.383e+02 |g| 2.243e-03 CG  23\n",
            "iter  6 act 2.799e-09 pre 2.797e-09 delta 2.173e+00 f 5.383e+02 |g| 2.156e-04 CG  26\n",
            "iter  7 act 1.808e-11 pre 1.976e-11 delta 2.173e+00 f 5.383e+02 |g| 2.141e-05 CG  22\n",
            "WARNING: actred and prered too small\n",
            "[FullSet] Va 0 logloss 0.706338\n",
            "[FullSet] Va 1 logloss 0.682088\n",
            "[FullSet] Va 2 logloss 0.703911\n",
            "[FullSet] Te logloss 0.666365\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# define the full-set-model \n",
        "\n",
        "\"\"\" \n",
        "model : logistic regression:\n",
        "C: Inverse of regularization strength. smaller values specify stronger regularization.\n",
        "fit_intercept: constant (a.k.a. bias or intercept) should be added to the decision function.\n",
        "tol: Tolerance for stopping criteria.\n",
        "solver: Algorithm to use in the optimization problem.For small datasets, ‘liblinear’ is a good choice\n",
        "multi_class: ‘ovr’, then a binary problem is fit for each label.\n",
        "max_iterint: Maximum number of iterations taken for the solvers to converge\n",
        "\"\"\"\n",
        "\n",
        "clf = LogisticRegression(\n",
        "        C = C,\n",
        "        fit_intercept=False,\n",
        "        tol = 1e-8,\n",
        "        solver=\"liblinear\",\n",
        "        multi_class=\"ovr\",\n",
        "        max_iter=100,\n",
        "        warm_start=False,\n",
        "        verbose=1,\n",
        "        )\n",
        "\n",
        "clf.fit(x_train_0,y_train_0)\n",
        "\n",
        "# on Va\n",
        "\n",
        "y_va_pred_0 = clf.predict_proba(x_va_0)[:,1] \n",
        "#predict_proba : Probability estimates. The returned estimates for all classes are ordered by the label of classes.\n",
        "full_logloss_0 = log_loss(y_va_0,y_va_pred_0) \n",
        "#Log loss, aka logistic loss or cross-entropy loss. This is the loss function defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true. \n",
        "#For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is\n",
        "#-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
        "weight_ar = clf.coef_.flatten() \n",
        "#Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary.\n",
        "\n",
        "\n",
        "# on Va\n",
        "clf.fit(x_train_1,y_train_1)\n",
        "y_va_pred_1 = clf.predict_proba(x_va_1)[:,1] \n",
        "full_logloss_1 = log_loss(y_va_1,y_va_pred_1) \n",
        "weight_ar = clf.coef_.flatten() \n",
        "\n",
        "# on Va\n",
        "clf.fit(x_train_2,y_train_2)\n",
        "y_va_pred_2 = clf.predict_proba(x_va_2)[:,1] \n",
        "full_logloss_2 = log_loss(y_va_2,y_va_pred_2) \n",
        "weight_ar = clf.coef_.flatten() \n",
        "\n",
        "# on Te\n",
        "\n",
        "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
        "full_te_logloss = log_loss(y_te,y_te_pred)\n",
        "full_te_auc = roc_auc_score(y_te, y_te_pred)\n",
        "#The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
        "# The true-positive rate is also known as sensitivity, recall or probability of detection\n",
        "# The false-positive rate is also known as probability of false alarm \n",
        "# AUC measures how true positive rate (recall) and false positive rate trade off\n",
        "\n",
        "y_te_pred = clf.predict(x_te)\n",
        "full_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
        "\n",
        "\n",
        "# print full-set-model results\n",
        "print(\"[FullSet] Va 0 logloss {:.6f}\".format(full_logloss_0))\n",
        "print(\"[FullSet] Va 1 logloss {:.6f}\".format(full_logloss_1))\n",
        "print(\"[FullSet] Va 2 logloss {:.6f}\".format(full_logloss_2))\n",
        "print(\"[FullSet] Te logloss {:.6f}\".format(full_te_logloss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ5h8v3kTwrh"
      },
      "source": [
        "# 4. **compute the influence function (IF) for each sample in training set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UeP4tgowUNvS"
      },
      "outputs": [],
      "source": [
        "def grad_logloss_theta_lr(label,ypred,x,C=0.03,has_l2=True,scale_factor=1.0):\n",
        "    \"\"\"Return d l_i / d_theta = d l_i / d_ypred * d y_pred / d theta\n",
        "        grad_logloss_theta: gradient on the theta, shape: [n,]\n",
        "    \"\"\"\n",
        "    # The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
        "    if not isinstance(label,np.ndarray) or not isinstance(ypred,np.ndarray):\n",
        "        label = np.array(label).flatten()\n",
        "        ypred = np.array(ypred).flatten()\n",
        "\n",
        "\n",
        "    grad_logloss_theta = C * x.T.dot(ypred-label)\n",
        "\n",
        "    return scale_factor * grad_logloss_theta\n",
        "\n",
        "def batch_grad_logloss_lr(label,ypred,x,C=0.03,scale_factor=1.0):\n",
        "    \"\"\"Return gradient on a batch.\n",
        "        batch_grad: gradient of each sample on parameters,\n",
        "            has shape [None,n]\n",
        "    \"\"\"\n",
        "    diffs = ypred - label\n",
        "    if isinstance(x,np.ndarray):\n",
        "        diffs = diffs.reshape(-1,1)\n",
        "        batch_grad = x * diffs\n",
        "    else:\n",
        "        diffs = sparse.diags(diffs)\n",
        "        batch_grad = x.T.dot(diffs).T\n",
        "    batch_grad = sparse.csr_matrix(C * batch_grad)      \n",
        "    return scale_factor * batch_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IhBwAgki1HAU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/inverse_hvp.py:146: OptimizeWarning: Unknown solver options: preconditioner\n",
            "  fmin_results = fmin_ncg(f=fmin_loss_fn,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Succeed in getting the inverse of preconditioner M.\n",
            "iter 0 cg iter 0 iter_diff 587.4367863336294\n",
            "Function value: -22105.499047340796\n",
            "Split function value: 22082.50548923874, -44188.00453657954\n",
            "iter 1 cg iter 0 iter_diff 245.4664914258141\n",
            "iter 1 cg iter 10 iter_diff 117.88006563606494\n",
            "Function value: -52845.013050906025\n",
            "Split function value: 32703.049231365305, -85548.06228227133\n",
            "iter 2 cg iter 0 iter_diff 143.82545381385643\n",
            "iter 2 cg iter 10 iter_diff 73.18450854578381\n",
            "Function value: -71647.38596220654\n",
            "Split function value: 74738.15882427718, -146385.54478648372\n",
            "iter 3 cg iter 0 iter_diff 51.05453151514966\n",
            "iter 3 cg iter 10 iter_diff 41.87249310542744\n",
            "Function value: -75175.99639309567\n",
            "Split function value: 70048.16999799918, -145224.16639109486\n",
            "iter 4 cg iter 0 iter_diff 21.212449961197855\n",
            "iter 4 cg iter 10 iter_diff 12.722533030674033\n",
            "Function value: -75628.79979547662\n",
            "Split function value: 75075.93144177685, -150704.73123725347\n",
            "iter 5 cg iter 0 iter_diff 8.84988334546823\n",
            "iter 5 cg iter 10 iter_diff 6.9510712311579805\n",
            "Function value: -75718.01934567976\n",
            "Split function value: 74424.33433021791, -150142.35367589767\n",
            "iter 6 cg iter 0 iter_diff 4.267973429852256\n",
            "iter 6 cg iter 10 iter_diff 2.641792569394021\n",
            "Function value: -75743.55794397062\n",
            "Split function value: 75927.86776376281, -151671.42570773343\n",
            "iter 7 cg iter 0 iter_diff 2.0151208675840113\n",
            "Function value: -75745.44691997927\n",
            "Split function value: 75666.83602793643, -151412.2829479157\n",
            "iter 8 cg iter 0 iter_diff 1.005941938550598\n",
            "iter 8 cg iter 10 iter_diff 0.6117547840937545\n",
            "Function value: -75746.27722583877\n",
            "Split function value: 75613.44737665079, -151359.72460248956\n",
            "iter 9 cg iter 0 iter_diff 0.44216478380902124\n",
            "iter 9 cg iter 10 iter_diff 0.29522531469135704\n",
            "Function value: -75746.70151725542\n",
            "Split function value: 75762.2259130383, -151508.92743029373\n",
            "iter 10 cg iter 0 iter_diff 0.16129325989311893\n",
            "Function value: -75746.71205575674\n",
            "Split function value: 75747.66937284931, -151494.38142860605\n",
            "iter 11 cg iter 0 iter_diff 0.06020897527084958\n",
            "iter 11 cg iter 10 iter_diff 0.03665064911308551\n",
            "iter 11 cg iter 20 iter_diff 0.011391859931637896\n",
            "Function value: -75746.71584609967\n",
            "Split function value: 75746.04656429468, -151492.76241039435\n",
            "iter 12 cg iter 0 iter_diff 0.011391859931918127\n",
            "iter 12 cg iter 10 iter_diff 0.002970980675905495\n",
            "iter 12 cg iter 20 iter_diff 0.0020528650867404448\n",
            "Function value: -75746.71595658593\n",
            "Split function value: 75746.6997146014, -151493.41567118734\n",
            "iter 13 cg iter 0 iter_diff 0.0008154924377868588\n",
            "iter 13 cg iter 10 iter_diff 0.0001752166009319951\n",
            "iter 13 cg iter 20 iter_diff 0.00020646295566983814\n",
            "Function value: -75746.71595710891\n",
            "Split function value: 75746.71517795412, -151493.43113506303\n",
            "iter 14 cg iter 0 iter_diff 1.2613780068504511e-05\n",
            "iter 14 cg iter 10 iter_diff 9.509729748369768e-06\n",
            "Function value: -75746.71595710919\n",
            "Split function value: 75746.71538644544, -151493.43134355464\n",
            "iter 15 cg iter 0 iter_diff 5.072541442326434e-06\n",
            "Function value: -75746.71595710922\n",
            "Split function value: 75746.7153865176, -151493.4313436268\n",
            "Optimization terminated successfully.\n",
            "         Current function value: -75746.715957\n",
            "         Iterations: 16\n",
            "         Function evaluations: 17\n",
            "         Gradient evaluations: 32\n",
            "         Hessian evaluations: 209\n",
            "Inverse HVP took 13.4 sec\n",
            "============================================================\n",
            "IF(inpulance function) Stats: mean -16.0901496560, max 363.6433535359, min -779.5516532161\n"
          ]
        }
      ],
      "source": [
        "# building precoditioner\n",
        "test_grad_loss_val = grad_logloss_theta_lr(y_va_0,y_va_pred_0,x_va_0,C,0.1/(num_tr_sample*C))  #Return d l_i / d_theta\n",
        "\n",
        "tr_pred_0 = clf.predict_proba(x_train_0)[:,1]\n",
        "batch_size = 10000\n",
        "M = None\n",
        "total_batch = int(np.ceil(num_tr_sample / float(batch_size)))\n",
        "\n",
        "for idx in range(total_batch):\n",
        "    batch_tr_grad = batch_grad_logloss_lr(y_train_0[idx*batch_size:(idx+1)*batch_size],\n",
        "        tr_pred_0[idx*batch_size:(idx+1)*batch_size],\n",
        "        x_train_0[idx*batch_size:(idx+1)*batch_size],\n",
        "        C,\n",
        "        1.0)\n",
        "\n",
        "    sum_grad = batch_tr_grad.multiply(x_train_0[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
        "    if M is None:\n",
        "        M = sum_grad\n",
        "    else:\n",
        "        M = M + sum_grad       \n",
        "M = M + 0.1/(num_tr_sample*C) * np.ones(x_train_0.shape[1])\n",
        "M = np.array(M).flatten()\n",
        "\n",
        "# computing the inverse Hessian-vector-product\n",
        "#The Hessian Matrix is a square matrix of second ordered partial derivatives of a scalar function.\n",
        "#It is of immense use in linear algebra as well as for determining points of local maxima or minima\n",
        "iv_hvp = inverse_hvp_lr_newtonCG(x_train_0,y_train_0,tr_pred_0,test_grad_loss_val,C,True,1e-5,True,M,0.1/(num_tr_sample*C))\n",
        "\n",
        "# get influence score\n",
        "total_batch = int(np.ceil(x_train_0.shape[0] / float(batch_size)))\n",
        "predicted_loss_diff = []\n",
        "for idx in range(total_batch):\n",
        "    train_grad_loss_val = batch_grad_logloss_lr(y_train_0[idx*batch_size:(idx+1)*batch_size],\n",
        "        tr_pred_0[idx*batch_size:(idx+1)*batch_size],\n",
        "        x_train_0[idx*batch_size:(idx+1)*batch_size],\n",
        "        C,\n",
        "        1.0)\n",
        "    predicted_loss_diff.extend(np.array(train_grad_loss_val.dot(iv_hvp)).flatten())    \n",
        "predicted_loss_diffs = np.asarray(predicted_loss_diff)\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"IF(inpulance function) Stats: mean {:.10f}, max {:.10f}, min {:.10f}\".format(\n",
        "    predicted_loss_diffs.mean(), predicted_loss_diffs.max(), predicted_loss_diffs.min())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmwNiFGmlJG"
      },
      "source": [
        "# **5.compute the sampling probability of each sample in training set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RBSf9aEgm1mb"
      },
      "outputs": [],
      "source": [
        "def select_from_one_class(y_train,prob_pi,label,ratio):\n",
        "    # select positive and negative samples respectively\n",
        "    num_sample = y_train[y_train==label].shape[0]\n",
        "    all_idx = np.arange(y_train.shape[0])[y_train==label]\n",
        "    label_prob_pi = prob_pi[all_idx]\n",
        "    obj_sample_size = int(ratio * num_sample)\n",
        "\n",
        "    sb_idx = None\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        rand_prob = np.random.rand(num_sample)\n",
        "        iter_idx = all_idx[rand_prob < label_prob_pi]\n",
        "        if sb_idx is None:\n",
        "            sb_idx = iter_idx\n",
        "        else:\n",
        "            new_idx = np.setdiff1d(iter_idx, sb_idx)\n",
        "            diff_size = obj_sample_size - sb_idx.shape[0]\n",
        "            if new_idx.shape[0] < diff_size:\n",
        "                sb_idx = np.union1d(iter_idx, sb_idx)\n",
        "            else:\n",
        "                new_idx = np.random.choice(new_idx, diff_size, replace=False)\n",
        "                sb_idx = np.union1d(sb_idx, new_idx)\n",
        "        iteration += 1\n",
        "        if sb_idx.shape[0] >= obj_sample_size:\n",
        "            sb_idx = np.random.choice(sb_idx,obj_sample_size,replace=False)\n",
        "            return sb_idx\n",
        "\n",
        "        if iteration > 100:\n",
        "            diff_size = obj_sample_size - sb_idx.shape[0]\n",
        "            leave_idx = np.setdiff1d(all_idx, sb_idx)\n",
        "            # left samples are sorted by their IF\n",
        "            # leave_idx = leave_idx[np.argsort(prob_pi[leave_idx])[-diff_size:]]\n",
        "            leave_idx = np.random.choice(leave_idx,diff_size,replace=False)\n",
        "            sb_idx = np.union1d(sb_idx, leave_idx)\n",
        "            return sb_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZND6k2RY1HAf",
        "outputId": "1f58ba1b-b708-40b5-8a01-0c37151ec95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pi Stats: [0.21331739 0.34073497 0.48384573 0.61182974 0.71756707]\n"
          ]
        }
      ],
      "source": [
        "# build sampling probability\n",
        "\n",
        "sigmoid_k = 10  # parameter for the sigmoid sampling function\n",
        "phi_ar = - predicted_loss_diffs\n",
        "IF_interval = phi_ar.max() - phi_ar.min()\n",
        "a_param = sigmoid_k / IF_interval\n",
        "prob_pi = 1 / (1 + np.exp(a_param * phi_ar))\n",
        "print(\"Pi Stats:\",np.percentile(prob_pi,[10,25,50,75,90]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJQYsVTynHWT"
      },
      "source": [
        "# **6. Do subsampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qThZ8V7snHvD"
      },
      "outputs": [],
      "source": [
        "def load_data_two_class(dataset_name,va_ratio):\n",
        "  x_train,y_train,x_test,y_test = load_mnist()\n",
        "  pos_class = 1\n",
        "  neg_class = 7\n",
        "  x_train,y_train = filter_dataset(x_train,y_train,pos_class,neg_class)\n",
        "  x_va,y_va = filter_dataset(x_test,y_test,pos_class,neg_class)\n",
        "  y_va = y_va.astype(int)\n",
        "  y_train = y_train.astype(int)\n",
        "\n",
        "  num_va_sample = int((1-va_ratio) * x_train.shape[0])\n",
        "  x_val = x_train[num_va_sample:]\n",
        "  y_val = y_train[num_va_sample:]\n",
        "  x_train = x_train[:num_va_sample]\n",
        "  y_train = y_train[:num_va_sample]\n",
        "  x_te = x_va\n",
        "  y_te = y_va\n",
        "\n",
        "  return x_train,y_train,x_val,y_val,x_te,y_te\n",
        "\n",
        "# load data, pick 30% as the Va set\n",
        "x_train,y_train,x_va,y_va,x_te,y_te = load_data_two_class(dataset_name,va_ratio=0.3)\n",
        "\n",
        "\n",
        "pos_idx = select_from_one_class(y_train_0,prob_pi,1,sample_ratio)\n",
        "neg_idx = select_from_one_class(y_train_0,prob_pi,0,sample_ratio)\n",
        "sb_idx = np.union1d(pos_idx,neg_idx)\n",
        "sb_x_train = x_train[sb_idx]\n",
        "sb_y_train = y_train[sb_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYgx4f4lnVXo"
      },
      "source": [
        "# **7. train a subset-model on the reduced data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xTdpYUwnVgo",
        "outputId": "13a5fa45-6ad2-41de-919d-fb0383e14485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LibLinear]iter  1 act 2.377e+02 pre 2.076e+02 delta 8.328e-01 f 3.297e+02 |g| 6.314e+02 CG   3\n",
            "cg reaches trust region boundary\n",
            "iter  2 act 5.145e+01 pre 4.116e+01 delta 1.114e+00 f 9.193e+01 |g| 1.668e+02 CG   5\n",
            "iter  3 act 1.805e+01 pre 1.432e+01 delta 1.114e+00 f 4.048e+01 |g| 6.080e+01 CG   5\n",
            "iter  4 act 6.261e+00 pre 5.049e+00 delta 1.114e+00 f 2.244e+01 |g| 2.293e+01 CG   5\n",
            "iter  5 act 1.828e+00 pre 1.510e+00 delta 1.114e+00 f 1.618e+01 |g| 8.533e+00 CG   5\n",
            "iter  6 act 3.958e-01 pre 3.454e-01 delta 1.114e+00 f 1.435e+01 |g| 3.026e+00 CG   5\n",
            "iter  7 act 3.443e-02 pre 3.316e-02 delta 1.114e+00 f 1.395e+01 |g| 8.108e-01 CG   5\n",
            "iter  8 act 5.498e-04 pre 5.492e-04 delta 1.114e+00 f 1.392e+01 |g| 7.772e-02 CG   6\n",
            "iter  9 act 5.671e-06 pre 5.671e-06 delta 1.114e+00 f 1.392e+01 |g| 5.497e-03 CG   7\n",
            "iter 10 act 1.348e-08 pre 1.348e-08 delta 1.114e+00 f 1.392e+01 |g| 2.919e-04 CG   7\n",
            "iter 11 act 4.812e-11 pre 4.810e-11 delta 1.114e+00 f 1.392e+01 |g| 1.673e-05 CG   7\n"
          ]
        }
      ],
      "source": [
        "clf.fit(sb_x_train,sb_y_train)\n",
        "y_va_pred = clf.predict_proba(x_va_2)[:,1]\n",
        "sb_logloss = log_loss(y_va_2, y_va_pred)\n",
        "sb_weight = clf.coef_.flatten()\n",
        "diff_w_norm = np.linalg.norm(weight_ar - sb_weight)\n",
        "sb_size = sb_x_train.shape[0]\n",
        "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
        "sb_te_logloss = log_loss(y_te,y_te_pred)\n",
        "sb_te_auc = roc_auc_score(y_te, y_te_pred)\n",
        "y_te_pred = clf.predict(x_te)\n",
        "sb_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTr5jkwnny2X"
      },
      "source": [
        "## **Result for comparision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6XxWsMo3xZ"
      },
      "source": [
        "## **Comparision of test accuracy on our UIDS technique with k cross  , before implementation without k cross**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwEqkYG01HAl",
        "outputId": "22252fa1-9398-4d42-a7ef-90e86700a73f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Result Summary on Te (ACC and AUC)\n",
            "[UIDS_k_fold]  acc 0.991678, auc 0.999424 # 4756\n",
            "[UIDS_before]   acc 0.984281, auc 0.998802 # 4994\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"==\"*30)\n",
        "print(\"Result Summary on Te (ACC and AUC)\")\n",
        "print(\"[UIDS_k_fold]  acc {:.6f}, auc {:.6f} # {}\".format(sb_te_acc,sb_te_auc, sb_size))\n",
        "print(\"[UIDS_before]   acc {:.6f}, auc {:.6f} # {}\".format(0.984281, 0.998802, 4994)) #taken from previous experiment where we do normal implementation of data without anf k cross validation\n",
        "print(\"==\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Improvement Unweighted Data Subsampling via Influence Function.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "86519fbbec93c191aacc49d4c33e96a49fd6ef772decd8151fefe193caa932bd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
