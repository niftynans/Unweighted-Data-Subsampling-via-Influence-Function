{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import gray2rgb, rgb2gray, label2rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = np.stack([gray2rgb(iimg) for iimg in mnist.data.values.reshape((-1, 28, 28))],0).astype(np.uint8)\n",
    "y_vec = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_te, y_train, y_te = train_test_split(X_vec, y_vec, test_size=0.2, random_state=1)\n",
    "x_train, x_va, y_train, y_va = train_test_split(x_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train, nr sample 42000, nr feature 28\n",
      "x_va,    nr sample 14000, nr feature 28\n",
      "x_te,    nr sample 14000, nr feature 28\n",
      "Tr: Pos 4701 Neg 4133\n",
      "Va: Pos 1544 Neg 1390\n",
      "Te: Pos 1632 Neg 1380\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train, nr sample {}, nr feature {}\".format(x_train.shape[0],x_train.shape[1]))\n",
    "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va.shape[0],x_va.shape[1]))\n",
    "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
    "print(\"Tr: Pos {} Neg {}\".format(y_train[y_train==1].shape[0],y_train[y_train==0].shape[0]))\n",
    "print(\"Va: Pos {} Neg {}\".format(y_va[y_va==1].shape[0],y_va[y_va==0].shape[0]))\n",
    "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Digit: 5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "ax1.imshow(X_vec[0], interpolation = 'none')\n",
    "ax1.set_title('Digit: {}'.format(y_vec[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class PipeStep(object):\n",
    "    \"\"\"\n",
    "    Wrapper for turning functions into pipeline transforms (no-fitting)\n",
    "    \"\"\"\n",
    "    def __init__(self, step_func):\n",
    "        self._step_func=step_func\n",
    "    def fit(self,*args):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return self._step_func(X)\n",
    "\n",
    "\n",
    "makegray_step = PipeStep(lambda img_list: [rgb2gray(img) for img in img_list])\n",
    "flatten_step = PipeStep(lambda img_list: [img.ravel() for img in img_list])\n",
    "\n",
    "simple_rf_pipeline = Pipeline([\n",
    "    ('Make Gray', makegray_step),\n",
    "    ('Flatten Image', flatten_step),\n",
    "    #('Normalize', Normalizer()),\n",
    "    #('PCA', PCA(16)),\n",
    "    ('RF', RandomForestClassifier())\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Make Gray', <__main__.PipeStep object at 0x7f06dc10f6d0>),\n",
       "                ('Flatten Image', <__main__.PipeStep object at 0x7f06dc10f670>),\n",
       "                ('RF', RandomForestClassifier())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rf_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training model on the full data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 10)\n",
      "[FullSet] Va logloss 0.277149\n",
      "[FullSet] Te logloss 0.280804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "y_va_pred = simple_rf_pipeline.predict_proba(x_va)# [:,1] #WHY DID THAT WORK?\n",
    "#predict_proba : Probability estimates. The returned estimates for all classes are ordered by the label of classes.\n",
    "print(y_va_pred.shape)\n",
    "full_logloss = log_loss(y_va,y_va_pred) \n",
    "\n",
    "#Log loss, aka logistic loss or cross-entropy loss. This is the loss function defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true. \n",
    "#For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is\n",
    "#-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "\n",
    "#weight_ar = simple_rf_pipeline.coef_.flatten() BE CAREFUL IF THIS IS BEING USED SOMEWHERE\n",
    "\n",
    "#Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary.\n",
    "\n",
    "# on Te\n",
    "\n",
    "y_te_pred = simple_rf_pipeline.predict_proba(x_te) #[:,1]\n",
    "full_te_logloss = log_loss(y_te,y_te_pred)\n",
    "full_te_auc = roc_auc_score(y_te, y_te_pred, multi_class='ovo')\n",
    "#The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "# The true-positive rate is also known as sensitivity, recall or probability of detection\n",
    "# The false-positive rate is also known as probability of false alarm \n",
    "# AUC measures how true positive rate (recall) and false positive rate trade off\n",
    "\n",
    "y_te_pred = simple_rf_pipeline.predict(x_te)\n",
    "full_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
    "\n",
    "\n",
    "# print full-set-model results\n",
    "print(\"[FullSet] Va logloss {:.6f}\".format(full_logloss))\n",
    "print(\"[FullSet] Te logloss {:.6f}\".format(full_te_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_va_pred_val = y_va_pred\n",
    "y_va_new = []\n",
    "\n",
    "for row in range(y_va_pred.shape[0]):\n",
    "    max_val = max(y_va_pred[row])\n",
    "\n",
    "    for i in range(len(y_va_pred[row])):\n",
    "\n",
    "        if y_va_pred[row][i] == max_val:\n",
    "            y_va_new.append(int(i))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_va_new = pd.Series(y_va_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Influence Value for each training sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logloss_theta_lr(label,ypred,x,C=0.03,has_l2=True,scale_factor=1.0):\n",
    "    \"\"\"Return d l_i / d_theta = d l_i / d_ypred * d y_pred / d theta\n",
    "        grad_logloss_theta: gradient on the theta, shape: [n,]\n",
    "    \"\"\"\n",
    "    # The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
    "    if not isinstance(label,np.ndarray) or not isinstance(ypred,np.ndarray):\n",
    "        label = np.array(label).flatten()\n",
    "        ypred = np.array(ypred).flatten()\n",
    "\n",
    "\n",
    "    grad_logloss_theta = C * x.T.dot(ypred-label)\n",
    "\n",
    "    return scale_factor * grad_logloss_theta\n",
    "\n",
    "def batch_grad_logloss_lr(label,ypred,x,C=0.03,scale_factor=1.0):\n",
    "    \"\"\"Return gradient on a batch.\n",
    "        batch_grad: gradient of each sample on parameters,\n",
    "            has shape [None,n]\n",
    "    \"\"\"\n",
    "    \n",
    "    # diffs = ypred - label\n",
    "    # for i in range(len(ypred)):\n",
    "    #     print(ypred[i] - label[i])\n",
    "    #     break\n",
    "\n",
    "    ypred = ypred.tolist()\n",
    "    label = label.tolist()\n",
    "    diffs = [x1 - x2 for (x1, x2) in zip(ypred, label)]\n",
    "\n",
    "    #diffs = ypred-label\n",
    "\n",
    "    diffs = pd.Series(diffs)\n",
    "    ypred = pd.Series(ypred)\n",
    "    label = pd.Series(label)\n",
    "    \n",
    "    # diffs = pd.Series(diffs)\n",
    "    # print(ypred.shape)\n",
    "    # print(label.shape)\n",
    "    # print(diffs.shape)\n",
    "    #print(diffs.isna().sum())\n",
    "    \n",
    "    \n",
    "    if isinstance(x,np.ndarray):\n",
    "        x = x.astype(np.float32) / 255\n",
    "        x = np.reshape(x, [x.shape[0], -1])\n",
    "\n",
    "        print(x.shape, diffs.shape)\n",
    "        diffs = diffs.values.reshape(-1,1)\n",
    "        batch_grad = x * diffs\n",
    "    else:\n",
    "        diffs = sparse.diags(diffs)\n",
    "        batch_grad = x.T.dot(diffs).T\n",
    "    batch_grad = sparse.csr_matrix(C * batch_grad)      \n",
    "    return scale_factor * batch_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2352) (10000,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10000,2352) (10000,28,28,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#print(tr_pred_new.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m batch_tr_grad \u001b[39m=\u001b[39m batch_grad_logloss_lr(y_train[idx\u001b[39m*\u001b[39mbatch_size:(idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m#tr_pred[idx*batch_size:(idx+1)*batch_size],\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     tr_pred_new,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     x_train[idx\u001b[39m*\u001b[39mbatch_size:(idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     C,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m sum_grad \u001b[39m=\u001b[39m batch_tr_grad\u001b[39m.\u001b[39;49mmultiply(x_train[idx\u001b[39m*\u001b[39;49mbatch_size:(idx\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49mbatch_size])\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m M \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     M \u001b[39m=\u001b[39m sum_grad\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/compressed.py:413\u001b[0m, in \u001b[0;36m_cs_matrix.multiply\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    410\u001b[0m other \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(other)\n\u001b[1;32m    412\u001b[0m \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mmultiply(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoarray(), other)\n\u001b[1;32m    414\u001b[0m \u001b[39m# Single element / wrapped object.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,2352) (10000,28,28,3) "
     ]
    }
   ],
   "source": [
    "# building precoditioner\n",
    "C = 0.1\n",
    "num_tr_sample = x_train.shape[0]\n",
    "\n",
    "\n",
    "test_grad_loss_val = grad_logloss_theta_lr(y_va,y_va_new,x_va,C ,0.1/(num_tr_sample*C))  #Return d l_i / d_theta\n",
    "\n",
    "tr_pred = simple_rf_pipeline.predict_proba(x_train) #[:,1]\n",
    "batch_size = 10000\n",
    "M = None\n",
    "total_batch = int(np.ceil(num_tr_sample / float(batch_size)))\n",
    "\n",
    "for idx in range(total_batch):\n",
    "    #print(y_train[idx*batch_size:(idx+1)*batch_size].shape)\n",
    "    #print(tr_pred[idx*batch_size:(idx+1)*batch_size].shape)\n",
    "\n",
    "    tr_pred_new = []\n",
    "\n",
    "    for row in range(tr_pred[idx*batch_size:(idx+1)*batch_size].shape[0]):\n",
    "        max_val = max(tr_pred[idx*batch_size:(idx+1)*batch_size][row])\n",
    "\n",
    "        for i in range(len(tr_pred[idx*batch_size:(idx+1)*batch_size][row])):\n",
    "\n",
    "            if tr_pred[idx*batch_size:(idx+1)*batch_size][row][i] == max_val:\n",
    "                tr_pred_new.append(int(i))\n",
    "                break\n",
    "    tr_pred_new = pd.Series(tr_pred_new)\n",
    "    #print(tr_pred_new.shape)\n",
    "\n",
    "    batch_tr_grad = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        #tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred_new,\n",
    "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        C,\n",
    "        1.0)\n",
    "\n",
    "    #sum_grad = batch_tr_grad.multiply(x_train[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
    "    sum_grad = batch_tr_grad.multiply(x_train[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
    "\n",
    "    if M is None:\n",
    "        M = sum_grad\n",
    "    else:\n",
    "        M = M + sum_grad       \n",
    "M = M + 0.1/(num_tr_sample*C) * np.ones(x_train.shape[1])\n",
    "M = np.array(M).flatten()\n",
    "\n",
    "# computing the inverse Hessian-vector-product\n",
    "#The Hessian Matrix is a square matrix of second ordered partial derivatives of a scalar function.\n",
    "#It is of immense use in linear algebra as well as for determining points of local maxima or minima\n",
    "iv_hvp = inverse_hvp_lr_newtonCG(x_train,y_train,tr_pred,test_grad_loss_val,C,True,1e-5,True,M,0.1/(num_tr_sample*C))\n",
    "\n",
    "# get influence score\n",
    "total_batch = int(np.ceil(x_train.shape[0] / float(batch_size)))\n",
    "predicted_loss_diff = []\n",
    "for idx in range(total_batch):\n",
    "    train_grad_loss_val = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        C,\n",
    "        1.0)\n",
    "    predicted_loss_diff.extend(np.array(train_grad_loss_val.dot(iv_hvp)).flatten())    \n",
    "predicted_loss_diffs = np.asarray(predicted_loss_diff)\n",
    "\n",
    "print(\"==\"*30)\n",
    "print(\"IF(influence function) Stats: mean {:.10f}, max {:.10f}, min {:.10f}\".format(\n",
    "    predicted_loss_diffs.mean(), predicted_loss_diffs.max(), predicted_loss_diffs.min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86519fbbec93c191aacc49d4c33e96a49fd6ef772decd8151fefe193caa932bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
