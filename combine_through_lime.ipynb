{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import gray2rgb, rgb2gray, label2rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = np.stack([gray2rgb(iimg) for iimg in mnist.data.values.reshape((-1, 28, 28))],0).astype(np.uint8)\n",
    "y_vec = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_te, y_train, y_te = train_test_split(X_vec, y_vec, test_size=0.2, random_state=1)\n",
    "x_train, x_va, y_train, y_va = train_test_split(x_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train, nr sample 42000, nr feature 28\n",
      "x_va,    nr sample 14000, nr feature 28\n",
      "x_te,    nr sample 14000, nr feature 28\n",
      "Tr: Pos 4701 Neg 4133\n",
      "Va: Pos 1544 Neg 1390\n",
      "Te: Pos 1632 Neg 1380\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train, nr sample {}, nr feature {}\".format(x_train.shape[0],x_train.shape[1]))\n",
    "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va.shape[0],x_va.shape[1]))\n",
    "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
    "print(\"Tr: Pos {} Neg {}\".format(y_train[y_train==1].shape[0],y_train[y_train==0].shape[0]))\n",
    "print(\"Va: Pos {} Neg {}\".format(y_va[y_va==1].shape[0],y_va[y_va==0].shape[0]))\n",
    "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Digit: 5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "ax1.imshow(X_vec[0], interpolation = 'none')\n",
    "ax1.set_title('Digit: {}'.format(y_vec[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class PipeStep(object):\n",
    "    \"\"\"\n",
    "    Wrapper for turning functions into pipeline transforms (no-fitting)\n",
    "    \"\"\"\n",
    "    def __init__(self, step_func):\n",
    "        self._step_func=step_func\n",
    "    def fit(self,*args):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return self._step_func(X)\n",
    "\n",
    "\n",
    "makegray_step = PipeStep(lambda img_list: [rgb2gray(img) for img in img_list])\n",
    "flatten_step = PipeStep(lambda img_list: [img.ravel() for img in img_list])\n",
    "\n",
    "simple_rf_pipeline = Pipeline([\n",
    "    ('Make Gray', makegray_step),\n",
    "    ('Flatten Image', flatten_step),\n",
    "    #('Normalize', Normalizer()),\n",
    "    #('PCA', PCA(16)),\n",
    "    ('RF', RandomForestClassifier())\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Make Gray', <__main__.PipeStep object at 0x7f06dc10f6d0>),\n",
       "                ('Flatten Image', <__main__.PipeStep object at 0x7f06dc10f670>),\n",
       "                ('RF', RandomForestClassifier())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rf_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training model on the full data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 10)\n",
      "[FullSet] Va logloss 0.277149\n",
      "[FullSet] Te logloss 0.280804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "y_va_pred = simple_rf_pipeline.predict_proba(x_va)# [:,1] #WHY DID THAT WORK?\n",
    "#predict_proba : Probability estimates. The returned estimates for all classes are ordered by the label of classes.\n",
    "print(y_va_pred.shape)\n",
    "full_logloss = log_loss(y_va,y_va_pred) \n",
    "\n",
    "#Log loss, aka logistic loss or cross-entropy loss. This is the loss function defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true. \n",
    "#For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is\n",
    "#-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "\n",
    "#weight_ar = simple_rf_pipeline.coef_.flatten() BE CAREFUL IF THIS IS BEING USED SOMEWHERE\n",
    "\n",
    "#Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary.\n",
    "\n",
    "# on Te\n",
    "\n",
    "y_te_pred = simple_rf_pipeline.predict_proba(x_te) #[:,1]\n",
    "full_te_logloss = log_loss(y_te,y_te_pred)\n",
    "full_te_auc = roc_auc_score(y_te, y_te_pred, multi_class='ovo')\n",
    "#The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "# The true-positive rate is also known as sensitivity, recall or probability of detection\n",
    "# The false-positive rate is also known as probability of false alarm \n",
    "# AUC measures how true positive rate (recall) and false positive rate trade off\n",
    "\n",
    "y_te_pred = simple_rf_pipeline.predict(x_te)\n",
    "full_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
    "\n",
    "\n",
    "# print full-set-model results\n",
    "print(\"[FullSet] Va logloss {:.6f}\".format(full_logloss))\n",
    "print(\"[FullSet] Te logloss {:.6f}\".format(full_te_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_va_pred_val = y_va_pred\n",
    "y_va_new = []\n",
    "\n",
    "for row in range(y_va_pred.shape[0]):\n",
    "    max_val = max(y_va_pred[row])\n",
    "\n",
    "    for i in range(len(y_va_pred[row])):\n",
    "\n",
    "        if y_va_pred[row][i] == max_val:\n",
    "            y_va_new.append(int(i))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_va_new = pd.Series(y_va_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Influence Value for each training sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logloss_theta_lr(label,ypred,x,C=0.03,has_l2=True,scale_factor=1.0):\n",
    "    \"\"\"Return d l_i / d_theta = d l_i / d_ypred * d y_pred / d theta\n",
    "        grad_logloss_theta: gradient on the theta, shape: [n,]\n",
    "    \"\"\"\n",
    "    # The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
    "    if not isinstance(label,np.ndarray) or not isinstance(ypred,np.ndarray):\n",
    "        label = np.array(label).flatten()\n",
    "        ypred = np.array(ypred).flatten()\n",
    "\n",
    "\n",
    "    grad_logloss_theta = C * x.T.dot(ypred-label)\n",
    "\n",
    "    return scale_factor * grad_logloss_theta\n",
    "\n",
    "def batch_grad_logloss_lr(label,ypred,x,C=0.03,scale_factor=1.0):\n",
    "    \"\"\"Return gradient on a batch.\n",
    "        batch_grad: gradient of each sample on parameters,\n",
    "            has shape [None,n]\n",
    "    \"\"\"\n",
    "    \n",
    "    # diffs = ypred - label\n",
    "    # for i in range(len(ypred)):\n",
    "    #     print(ypred[i] - label[i])\n",
    "    #     break\n",
    "\n",
    "    ypred = ypred.tolist()\n",
    "    label = label.tolist()\n",
    "    diffs = [x1 - x2 for (x1, x2) in zip(ypred, label)]\n",
    "\n",
    "    #diffs = ypred-label\n",
    "\n",
    "    diffs = pd.Series(diffs)\n",
    "    ypred = pd.Series(ypred)\n",
    "    label = pd.Series(label)\n",
    "    \n",
    "    # diffs = pd.Series(diffs)\n",
    "    # print(ypred.shape)\n",
    "    # print(label.shape)\n",
    "    # print(diffs.shape)\n",
    "    #print(diffs.isna().sum())\n",
    "    \n",
    "    \n",
    "    if isinstance(x,np.ndarray):\n",
    "        x = x.astype(np.float32) / 255\n",
    "        x = np.reshape(x, [x.shape[0], -1])\n",
    "\n",
    "        #print(x.shape, diffs.shape)\n",
    "        diffs = diffs.values.reshape(-1,1)\n",
    "        batch_grad = x * diffs\n",
    "    else:\n",
    "        diffs = sparse.diags(diffs)\n",
    "        batch_grad = x.T.dot(diffs).T\n",
    "    batch_grad = sparse.csr_matrix(C * batch_grad)      \n",
    "    return scale_factor * batch_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 5\n",
      "(10000, 2352)\n",
      "(10000, 28, 28, 3)\n",
      "REACHED\n",
      "1 / 5\n",
      "(10000, 2352)\n",
      "(10000, 28, 28, 3)\n",
      "REACHED\n",
      "2 / 5\n",
      "(10000, 2352)\n",
      "(10000, 28, 28, 3)\n",
      "REACHED\n",
      "3 / 5\n",
      "(10000, 2352)\n",
      "(10000, 28, 28, 3)\n",
      "REACHED\n",
      "4 / 5\n",
      "(2000, 2352)\n",
      "(2000, 28, 28, 3)\n",
      "REACHED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/inverse_hvp.py:146: OptimizeWarning: Unknown solver options: preconditioner\n",
      "  fmin_results = fmin_ncg(f=fmin_loss_fn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeed in getting the inverse of preconditioner M.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (42000,28,28,3) and (2352,) not aligned: 3 (dim 3) != 2352 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minverse_hvp\u001b[39;00m \u001b[39mimport\u001b[39;00m inverse_hvp_lr_newtonCG\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m iv_hvp \u001b[39m=\u001b[39m inverse_hvp_lr_newtonCG(x_train,y_train,tr_pred,test_grad_loss_val,C,\u001b[39mTrue\u001b[39;49;00m,\u001b[39m1e-5\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m,M,\u001b[39m0.1\u001b[39;49m\u001b[39m/\u001b[39;49m(num_tr_sample\u001b[39m*\u001b[39;49mC))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# get influence score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pnans/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/combine_through_lime.ipynb#X22sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m total_batch \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(batch_size)))\n",
      "File \u001b[0;32m~/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/inverse_hvp.py:146\u001b[0m, in \u001b[0;36minverse_hvp_lr_newtonCG\u001b[0;34m(x_train, y_train, y_pred, v, C, hessian_free, tol, has_l2, M, scale_factor)\u001b[0m\n\u001b[1;32m    144\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    145\u001b[0m cg_callback \u001b[39m=\u001b[39m get_cg_callback(verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 146\u001b[0m fmin_results \u001b[39m=\u001b[39m fmin_ncg(f\u001b[39m=\u001b[39;49mfmin_loss_fn,\n\u001b[1;32m    147\u001b[0m                        x0\u001b[39m=\u001b[39;49mv,\n\u001b[1;32m    148\u001b[0m                        fprime\u001b[39m=\u001b[39;49mfmin_grad_fn,\n\u001b[1;32m    149\u001b[0m                        fhess_p\u001b[39m=\u001b[39;49mget_fmin_hvp,\n\u001b[1;32m    150\u001b[0m                        callback\u001b[39m=\u001b[39;49mcg_callback,\n\u001b[1;32m    151\u001b[0m                        avextol\u001b[39m=\u001b[39;49mtol,\n\u001b[1;32m    152\u001b[0m                        maxiter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m    153\u001b[0m                        preconditioner\u001b[39m=\u001b[39;49mM)\n\u001b[1;32m    154\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInverse HVP took \u001b[39m\u001b[39m{:.1f}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time))\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m fmin_results\n",
      "File \u001b[0;32m~/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/optimize.py:276\u001b[0m, in \u001b[0;36mfmin_ncg\u001b[0;34m(f, x0, fprime, fhess_p, fhess, args, avextol, epsilon, maxiter, full_output, disp, retall, callback, preconditioner)\u001b[0m\n\u001b[1;32m    267\u001b[0m     preconditioner \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mones(x0\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    269\u001b[0m opts \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mxtol\u001b[39m\u001b[39m'\u001b[39m: avextol,\n\u001b[1;32m    270\u001b[0m         \u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m: epsilon,\n\u001b[1;32m    271\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: maxiter,\n\u001b[1;32m    272\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp,\n\u001b[1;32m    273\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreturn_all\u001b[39m\u001b[39m'\u001b[39m: retall,\n\u001b[1;32m    274\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mpreconditioner\u001b[39m\u001b[39m'\u001b[39m: preconditioner,}\n\u001b[0;32m--> 276\u001b[0m res \u001b[39m=\u001b[39m _minimize_newton_pcg(f, x0, args, fprime, fhess, fhess_p,callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n\u001b[1;32m    277\u001b[0m \u001b[39m# res = _minimize_newtoncg(f, x0, args, fprime, fhess, fhess_p,callback=callback, **opts)\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m full_output:\n",
      "File \u001b[0;32m~/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/optimize.py:367\u001b[0m, in \u001b[0;36m_minimize_newton_pcg\u001b[0;34m(fun, x0, args, jac, hess, hessp, callback, xtol, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    365\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    366\u001b[0m gfk \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m old_fval \u001b[39m=\u001b[39m f(x0)\n\u001b[1;32m    368\u001b[0m old_old_fval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    369\u001b[0m float64eps \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfinfo(numpy\u001b[39m.\u001b[39mfloat64)\u001b[39m.\u001b[39meps\n",
      "File \u001b[0;32m~/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/optimize.py:120\u001b[0m, in \u001b[0;36mwrap_function.<locals>.function_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunction_wrapper\u001b[39m(\u001b[39m*\u001b[39mwrapper_args):\n\u001b[1;32m    119\u001b[0m     ncalls[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49m(wrapper_args \u001b[39m+\u001b[39;49m args))\n",
      "File \u001b[0;32m~/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/inverse_hvp.py:96\u001b[0m, in \u001b[0;36minverse_hvp_lr_newtonCG.<locals>.fmin_loss_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\"\"\"Objective function for newton-cg.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mH^-1 * v = argmin_t {0.5 * t^T * H * t - v^T * t}\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m hessian_free:\n\u001b[0;32m---> 96\u001b[0m     hessian_vec_val \u001b[39m=\u001b[39m hessian_vector_product_lr(y_train,y_pred,x_train,x,C,has_l2,scale_factor) \u001b[39m# [n,]\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     hessian_vec_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x,hessian_matrix) \u001b[39m# [n,]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Unweighted-Data-Subsampling-via-Influence-Function/inverse_hvp.py:71\u001b[0m, in \u001b[0;36mhessian_vector_product_lr\u001b[0;34m(label, ypred, x, v, C, has_l2, scale_factor)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhessian_vector_product_lr\u001b[39m(label,ypred,x,v,C\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m,has_l2\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,scale_factor\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[39m\"\"\"Get implicit hessian-vector-products without explicitly building the hessian.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    H*v = v + C *X^T*(D*(X*v))\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     xv \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mdot(v)\n\u001b[1;32m     72\u001b[0m     D \u001b[39m=\u001b[39m ypred \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ypred)\n\u001b[1;32m     73\u001b[0m     dxv \u001b[39m=\u001b[39m xv \u001b[39m*\u001b[39m D\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (42000,28,28,3) and (2352,) not aligned: 3 (dim 3) != 2352 (dim 0)"
     ]
    }
   ],
   "source": [
    "# building precoditioner\n",
    "from turtle import goto\n",
    "\n",
    "C = 0.1\n",
    "num_tr_sample = x_train.shape[0]\n",
    "\n",
    "\n",
    "test_grad_loss_val = grad_logloss_theta_lr(y_va,y_va_new,x_va,C ,0.1/(num_tr_sample*C))  #Return d l_i / d_theta\n",
    "\n",
    "tr_pred = simple_rf_pipeline.predict_proba(x_train) #[:,1]\n",
    "batch_size = 10000\n",
    "M = None\n",
    "total_batch = int(np.ceil(num_tr_sample / float(batch_size)))\n",
    "\n",
    "for idx in range(total_batch+1):\n",
    "    print(idx, '/',total_batch)\n",
    "    #print(y_train[idx*batch_size:(idx+1)*batch_size].shape)\n",
    "    #print(tr_pred[idx*batch_size:(idx+1)*batch_size].shape)\n",
    "\n",
    "    tr_pred_new = []\n",
    "\n",
    "    for row in range(tr_pred[idx*batch_size:(idx+1)*batch_size].shape[0]):\n",
    "        max_val = max(tr_pred[idx*batch_size:(idx+1)*batch_size][row])\n",
    "\n",
    "        for i in range(len(tr_pred[idx*batch_size:(idx+1)*batch_size][row])):\n",
    "\n",
    "            if tr_pred[idx*batch_size:(idx+1)*batch_size][row][i] == max_val:\n",
    "                tr_pred_new.append(int(i))\n",
    "                break\n",
    "    tr_pred_new = pd.Series(tr_pred_new)\n",
    "    #print(tr_pred_new.shape)\n",
    "\n",
    "    batch_tr_grad = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        #tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred_new,\n",
    "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        C,\n",
    "        1.0)\n",
    "\n",
    "    #batch_tr_grad_x_train = x_train[idx*batch_size:(idx+1)*batch_size].reshape((10000,2352))\n",
    "    #sum_grad = batch_tr_grad.multiply(x_train[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
    "    print(batch_tr_grad.shape)\n",
    "    print(x_train[idx*batch_size:(idx+1)*batch_size].shape)\n",
    "    batch_tr_grad_x_train = np.reshape(x_train[idx*batch_size:(idx+1)*batch_size],(batch_tr_grad.shape))\n",
    "\n",
    "    print(\"REACHED\")\n",
    "    sum_grad = batch_tr_grad.multiply(batch_tr_grad_x_train).sum(0)\n",
    "\n",
    "    if M is None:\n",
    "        M = sum_grad\n",
    "    else:\n",
    "        M = M + sum_grad \n",
    "    if idx == 4:\n",
    "        break    \n",
    "\n",
    "hess_grad_shape = x_train.shape[1] * x_train.shape[2] * x_train.shape[3]\n",
    "M = M + 0.1/(num_tr_sample*C) * np.ones(hess_grad_shape)\n",
    "M = np.array(M).flatten()\n",
    "\n",
    "# computing the inverse Hessian-vector-product\n",
    "#The Hessian Matrix is a square matrix of second ordered partial derivatives of a scalar function.\n",
    "#It is of immense use in linear algebra as well as for determining points of local maxima or minima\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from inverse_hvp import inverse_hvp_lr_newtonCG\n",
    "\n",
    "iv_hvp = inverse_hvp_lr_newtonCG(x_train,y_train,tr_pred,test_grad_loss_val,C,True,1e-5,True,M,0.1/(num_tr_sample*C))\n",
    "\n",
    "# get influence score\n",
    "total_batch = int(np.ceil(x_train.shape[0] / float(batch_size)))\n",
    "predicted_loss_diff = []\n",
    "for idx in range(total_batch):\n",
    "    train_grad_loss_val = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        C,\n",
    "        1.0)\n",
    "    predicted_loss_diff.extend(np.array(train_grad_loss_val.dot(iv_hvp)).flatten())    \n",
    "predicted_loss_diffs = np.asarray(predicted_loss_diff)\n",
    "\n",
    "print(\"==\"*30)\n",
    "print(\"IF(influence function) Stats: mean {:.10f}, max {:.10f}, min {:.10f}\".format(\n",
    "    predicted_loss_diffs.mean(), predicted_loss_diffs.max(), predicted_loss_diffs.min())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86519fbbec93c191aacc49d4c33e96a49fd6ef772decd8151fefe193caa932bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
